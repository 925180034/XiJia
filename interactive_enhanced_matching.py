# """
# äº¤äº’å¼å¢å¼ºSchemaåŒ¹é…è„šæœ¬ - æ”¯æŒåŠ¨æ€å‚æ•°è°ƒæ•´
# """
# import os
# import sys
# import yaml
# import pandas as pd
# import numpy as np
# from typing import Dict, List, Tuple, Any, Optional
# import time
# import argparse
# import json
# import re
# import requests
# from tqdm import tqdm

# from src.data.data_loader import DataLoader, SchemaMetadata
# from src.data.data_preprocessor import MetadataPreprocessor
# from src.features.enhanced_similarity_calculator import EnhancedSimilarityCalculator
# from src.matching.candidate_filter import CandidateFilter
# from src.matching.result_processor import ResultProcessor


# class InteractiveEnhancedLLMMatcher:
#     """äº¤äº’å¼å¢å¼ºLLMåŒ¹é…å™¨"""
    
#     def __init__(self, config_path: str = "config/config_enhanced.yaml"):
#         print(f"åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
        
#         with open(config_path, "r", encoding="utf-8") as f:
#             self.config = yaml.safe_load(f)
        
#         self.api_key = self.config["openai"]["api_key"]
#         self.api_base_url = self.config["openai"]["api_base_url"]
#         self.model = self.config["openai"]["model"]
#         self.temperature = self.config["openai"]["temperature"]
#         self.max_tokens = self.config["openai"]["max_tokens"]
        
#         # å¯åŠ¨æ€è°ƒæ•´çš„å‚æ•°
#         self.batch_size = self.config["system"]["batch_size"]
#         self.cache_enabled = self.config["system"]["cache_enabled"]
#         self.enable_aggressive_mode = self.config["matching_strategy"]["enable_aggressive_mode"]
        
#         # é˜ˆå€¼å‚æ•°ï¼ˆå¯åŠ¨æ€è°ƒæ•´ï¼‰
#         self.similarity_threshold = self.config["thresholds"]["similarity_threshold"]
#         self.high_confidence_threshold = self.config["thresholds"]["high_confidence"]
#         self.medium_confidence_threshold = self.config["thresholds"]["medium_confidence"]
#         self.low_confidence_threshold = self.config["thresholds"]["low_confidence"]
        
#         print(f"APIé…ç½®: {self.api_base_url}, æ¨¡å‹: {self.model}")
#         print(f"å½“å‰é˜ˆå€¼é…ç½®:")
#         print(f"  ç›¸ä¼¼åº¦é˜ˆå€¼: {self.similarity_threshold}")
#         print(f"  é«˜ç½®ä¿¡åº¦é˜ˆå€¼: {self.high_confidence_threshold}")
#         print(f"  ä¸­ç­‰ç½®ä¿¡åº¦é˜ˆå€¼: {self.medium_confidence_threshold}")
#         print(f"  ä½ç½®ä¿¡åº¦é˜ˆå€¼: {self.low_confidence_threshold}")
        
#         # ç¼“å­˜
#         self._cache = {}
#         self._cache_path = "cache/interactive_enhanced_responses.json"
#         if self.cache_enabled:
#             os.makedirs(os.path.dirname(self._cache_path), exist_ok=True)
#             if os.path.exists(self._cache_path):
#                 with open(self._cache_path, "r", encoding="utf-8") as f:
#                     self._cache = json.load(f)
        
#         self.success_count = 0
#         self.fail_count = 0
#         self.cache_hit_count = 0
    
#     def update_thresholds(self, similarity_threshold=None, high_conf=None, 
#                          medium_conf=None, low_conf=None):
#         """åŠ¨æ€æ›´æ–°é˜ˆå€¼"""
#         if similarity_threshold is not None:
#             self.similarity_threshold = similarity_threshold
#             print(f"âœ… ç›¸ä¼¼åº¦é˜ˆå€¼æ›´æ–°ä¸º: {self.similarity_threshold}")
        
#         if high_conf is not None:
#             self.high_confidence_threshold = high_conf
#             print(f"âœ… é«˜ç½®ä¿¡åº¦é˜ˆå€¼æ›´æ–°ä¸º: {self.high_confidence_threshold}")
        
#         if medium_conf is not None:
#             self.medium_confidence_threshold = medium_conf
#             print(f"âœ… ä¸­ç­‰ç½®ä¿¡åº¦é˜ˆå€¼æ›´æ–°ä¸º: {self.medium_confidence_threshold}")
        
#         if low_conf is not None:
#             self.low_confidence_threshold = low_conf
#             print(f"âœ… ä½ç½®ä¿¡åº¦é˜ˆå€¼æ›´æ–°ä¸º: {self.low_confidence_threshold}")
    
#     def update_matching_settings(self, batch_size=None, aggressive_mode=None):
#         """åŠ¨æ€æ›´æ–°åŒ¹é…è®¾ç½®"""
#         if batch_size is not None:
#             self.batch_size = batch_size
#             print(f"âœ… æ‰¹å¤„ç†å¤§å°æ›´æ–°ä¸º: {self.batch_size}")
        
#         if aggressive_mode is not None:
#             self.enable_aggressive_mode = aggressive_mode
#             print(f"âœ… ç§¯æåŒ¹é…æ¨¡å¼: {'å¯ç”¨' if self.enable_aggressive_mode else 'ç¦ç”¨'}")
    
#     def _call_api_like_test(self, prompt: str) -> str:
#         """APIè°ƒç”¨æ–¹æ³•"""
#         try:
#             api_url = f"{self.api_base_url}/chat/completions"
            
#             headers = {
#                 "Content-Type": "application/json",
#                 "Authorization": f"Bearer {self.api_key}"
#             }
            
#             data = {
#                 "model": self.model,
#                 "messages": [{"role": "user", "content": prompt}],
#                 "temperature": self.temperature,
#                 "max_tokens": self.max_tokens
#             }
            
#             response = requests.post(api_url, headers=headers, json=data, timeout=30)
            
#             if response.status_code != 200:
#                 return f"é”™è¯¯ï¼šAPIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}"
            
#             resp_json = response.json()
            
#             if "choices" in resp_json and len(resp_json["choices"]) > 0:
#                 choice = resp_json["choices"][0]
#                 if "message" in choice and "content" in choice["message"]:
#                     content = choice["message"]["content"]
#                     if content and content.strip():
#                         return content
#                     else:
#                         return "é”™è¯¯ï¼šAPIè¿”å›ç©ºå†…å®¹"
#                 elif "text" in choice:
#                     return choice["text"]
#                 else:
#                     return f"é”™è¯¯ï¼šå“åº”æ ¼å¼å¼‚å¸¸"
#             else:
#                 return f"é”™è¯¯ï¼šå“åº”ä¸­æ²¡æœ‰choiceså­—æ®µ"
                
#         except requests.exceptions.Timeout:
#             return "é”™è¯¯ï¼šè¯·æ±‚è¶…æ—¶"
#         except requests.exceptions.ConnectionError:
#             return "é”™è¯¯ï¼šè¿æ¥é”™è¯¯"
#         except Exception as e:
#             return f"é”™è¯¯ï¼š{str(e)}"
    
#     def match_field_pair(self, source_field: Dict, target_field: Dict, similarity: float) -> Dict:
#         """åŒ¹é…å•å¯¹å­—æ®µ"""
#         cache_key = f"interactive_{source_field['name']}_{target_field['name']}_{similarity:.3f}"
        
#         if self.cache_enabled and cache_key in self._cache:
#             self.cache_hit_count += 1
#             return self._cache[cache_key]
        
#         # æ ¹æ®ç›¸ä¼¼åº¦è°ƒæ•´æç¤ºç­–ç•¥
#         if similarity >= 0.8:
#             prompt_type = "high_similarity"
#         elif similarity >= 0.5:
#             prompt_type = "medium_similarity"
#         else:
#             prompt_type = "low_similarity"
        
#         prompt = self._create_adaptive_prompt(source_field, target_field, similarity, prompt_type)
        
#         response = self._call_api_like_test(prompt)
        
#         if response.startswith("é”™è¯¯ï¼š"):
#             self.fail_count += 1
#             result = self._generate_fallback_result(source_field, target_field, similarity, response)
#         else:
#             self.success_count += 1
#             result = self._parse_response(response, similarity)
        
#         result["similarity"] = similarity
        
#         if self.cache_enabled:
#             self._cache[cache_key] = result
#             if len(self._cache) % 10 == 0:
#                 self._save_cache()
        
#         return result
    
#     def _create_adaptive_prompt(self, source_field: Dict, target_field: Dict, 
#                                similarity: float, prompt_type: str) -> str:
#         """æ ¹æ®ç›¸ä¼¼åº¦åˆ›å»ºè‡ªé€‚åº”æç¤º"""
        
#         if prompt_type == "high_similarity":
#             # é«˜ç›¸ä¼¼åº¦ï¼šè¯¦ç»†åˆ†æ
#             prompt = f"""ç³»ç»Ÿè§’è‰²ï¼šæ‚¨æ˜¯æ•°æ®é›†æˆä¸“å®¶ï¼Œæ“…é•¿è¯†åˆ«å­—æ®µé—´çš„è¯­ä¹‰ç­‰ä»·å…³ç³»ã€‚

# ä»»åŠ¡æè¿°ï¼šåˆ¤æ–­ä¸¤ä¸ªå­—æ®µæ˜¯å¦è¯­ä¹‰ç­‰ä»·ã€‚è¿™ä¸¤ä¸ªå­—æ®µå…·æœ‰è¾ƒé«˜çš„ç›¸ä¼¼åº¦({similarity:.2f})ï¼Œè¯·ä»”ç»†åˆ†æå®ƒä»¬æ˜¯å¦è¡¨ç¤ºç›¸åŒçš„ä¸šåŠ¡æ¦‚å¿µã€‚

# æºå­—æ®µï¼š
# - å­—æ®µåï¼š{source_field['name']}
# - å­—æ®µæè¿°ï¼š{source_field.get('desc', '(æ— )')}
# - å­—æ®µç±»å‹ï¼š{source_field.get('type', '(æœªçŸ¥)')}

# ç›®æ ‡å­—æ®µï¼š
# - å­—æ®µåï¼š{target_field['name']}
# - å­—æ®µæè¿°ï¼š{target_field.get('desc', '(æ— )')}
# - å­—æ®µç±»å‹ï¼š{target_field.get('type', '(æœªçŸ¥)')}

# åˆ†æè¦ç‚¹ï¼š
# 1. å­—æ®µåè¯­ä¹‰å…³ç³»ï¼šè€ƒè™‘ä¸­è‹±æ–‡å¯¹åº”ã€æ‹¼éŸ³ç¼©å†™ï¼ˆå¦‚"XSBH"="å­¦ç”Ÿç¼–å·"å¯¹åº”"XH"="å­¦å·"ï¼‰
# 2. ä¸šåŠ¡æ¦‚å¿µåŒ¹é…ï¼šåˆ†æå­—æ®µåœ¨ä¸šåŠ¡æµç¨‹ä¸­çš„ä½œç”¨æ˜¯å¦ç›¸åŒ
# 3. æè¿°è¯­ä¹‰ï¼šæ¯”è¾ƒå­—æ®µæè¿°çš„è¯­ä¹‰ç›¸ä¼¼åº¦
# 4. æ•°æ®ç±»å‹å…¼å®¹æ€§ï¼šæ£€æŸ¥æ•°æ®ç±»å‹æ˜¯å¦å…¼å®¹

# åˆ¤æ–­æ ‡å‡†ï¼š
# - å¦‚æœä¸¤ä¸ªå­—æ®µè¡¨ç¤ºç›¸åŒçš„ä¸šåŠ¡æ¦‚å¿µï¼Œå³ä½¿åç§°ä¸å®Œå…¨ç›¸åŒï¼Œä¹Ÿåº”è¯¥åˆ¤æ–­ä¸ºåŒ¹é…
# - ä¸­è‹±æ–‡å­—æ®µå¦‚æœè¯­ä¹‰ç›¸åŒï¼Œåº”åˆ¤æ–­ä¸ºåŒ¹é…
# - æ‹¼éŸ³ç¼©å†™ä¸åŸè¯å¦‚æœå¯¹åº”ï¼Œåº”åˆ¤æ–­ä¸ºåŒ¹é…

# è¯·ç»™å‡ºåˆ¤æ–­å’Œç½®ä¿¡åº¦ï¼š
# æ ¼å¼ï¼š
# åˆ¤æ–­ï¼š[æ˜¯/å¦]
# ç½®ä¿¡åº¦ï¼š[0-1ä¹‹é—´çš„æ•°å­—]
# ç†ç”±ï¼š[è¯¦ç»†è¯´æ˜]"""

#         elif prompt_type == "medium_similarity":
#             # ä¸­ç­‰ç›¸ä¼¼åº¦ï¼šé‡ç‚¹åˆ†æ
#             prompt = f"""ç³»ç»Ÿè§’è‰²ï¼šæ‚¨æ˜¯æ•°æ®é›†æˆä¸“å®¶ï¼Œæ“…é•¿åœ¨å¤æ‚æƒ…å†µä¸‹è¯†åˆ«å­—æ®µé—´çš„æ½œåœ¨ç­‰ä»·å…³ç³»ã€‚

# ä»»åŠ¡æè¿°ï¼šåˆ¤æ–­ä¸¤ä¸ªå­—æ®µæ˜¯å¦å¯èƒ½è¯­ä¹‰ç­‰ä»·ã€‚è¿™ä¸¤ä¸ªå­—æ®µå…·æœ‰ä¸­ç­‰ç›¸ä¼¼åº¦({similarity:.2f})ï¼Œéœ€è¦æ·±å…¥åˆ†æä¸šåŠ¡è¯­ä¹‰ã€‚

# æºå­—æ®µï¼š{source_field['name']} ({source_field.get('desc', 'æ— æè¿°')})
# ç›®æ ‡å­—æ®µï¼š{target_field['name']} ({target_field.get('desc', 'æ— æè¿°')})

# æ·±åº¦åˆ†æè¦ç‚¹ï¼š
# 1. è¯­ä¹‰æŒ–æ˜ï¼šå³ä½¿åç§°ä¸åŒï¼Œåˆ†ææ˜¯å¦è¡¨ç¤ºç›¸åŒä¸šåŠ¡æ¦‚å¿µ
# 2. æ¨¡ç³ŠåŒ¹é…ï¼šè€ƒè™‘åŒä¹‰è¯ã€è¿‘ä¹‰è¯ã€ç¼©å†™å½¢å¼
# 3. é¢†åŸŸçŸ¥è¯†ï¼šè¿ç”¨æ•™è‚²ã€ç®¡ç†ç­‰é¢†åŸŸçš„å¸¸è¯†åˆ¤æ–­

# åŒ¹é…å€¾å‘ï¼š
# - ä¼˜å…ˆè€ƒè™‘ä¸šåŠ¡è¯­ä¹‰ç›¸ä¼¼æ€§
# - å¯¹äºå¯èƒ½çš„åŒ¹é…ï¼Œç»™äºˆç§¯æçš„åˆ¤æ–­

# è¯·ç»™å‡ºåˆ¤æ–­å’Œç½®ä¿¡åº¦ï¼š
# æ ¼å¼ï¼š
# åˆ¤æ–­ï¼š[æ˜¯/å¦]
# ç½®ä¿¡åº¦ï¼š[0-1ä¹‹é—´çš„æ•°å­—]
# ç†ç”±ï¼š[è¯¦ç»†è¯´æ˜]"""

#         else:
#             # ä½ç›¸ä¼¼åº¦ï¼šç®€åŒ–åˆ†æ
#             prompt = f"""åˆ¤æ–­ä¸¤ä¸ªæ•°æ®åº“å­—æ®µæ˜¯å¦è¯­ä¹‰ç­‰ä»·ï¼š

# æºå­—æ®µï¼š{source_field['name']} ({source_field.get('desc', 'æ— æè¿°')})
# ç›®æ ‡å­—æ®µï¼š{target_field['name']} ({target_field.get('desc', 'æ— æè¿°')})
# ç›¸ä¼¼åº¦ï¼š{similarity:.2f}

# åˆ†æè¦ç‚¹ï¼š
# 1. å­—æ®µåè¯­ä¹‰å…³ç³»ï¼ˆè€ƒè™‘ä¸­è‹±æ–‡å¯¹åº”ã€æ‹¼éŸ³ç¼©å†™ï¼‰
# 2. ä¸šåŠ¡æ¦‚å¿µæ˜¯å¦ç›¸åŒ
# 3. æè¿°å†…å®¹åŒ¹é…åº¦

# å›ç­”æ ¼å¼ï¼š
# åˆ¤æ–­ï¼šæ˜¯/å¦
# ç½®ä¿¡åº¦ï¼š0-1æ•°å­—
# ç†ç”±ï¼šç®€è¦è¯´æ˜"""
        
#         return prompt
    
#     def _parse_response(self, response: str, similarity: float) -> Dict:
#         """è§£æLLMå“åº”"""
#         try:
#             match = False
#             if any(indicator in response for indicator in ["åˆ¤æ–­ï¼šæ˜¯", "åˆ¤æ–­:æ˜¯", "åˆ¤æ–­: æ˜¯"]):
#                 match = True
            
#             confidence = 0.0
#             patterns = [r"ç½®ä¿¡åº¦ï¼š([0-9.]+)", r"ç½®ä¿¡åº¦:([0-9.]+)", r"ç½®ä¿¡åº¦: ([0-9.]+)"]
            
#             for pattern in patterns:
#                 match_obj = re.search(pattern, response)
#                 if match_obj:
#                     try:
#                         confidence = float(match_obj.group(1))
#                         break
#                     except ValueError:
#                         pass
            
#             if confidence == 0.0:
#                 if match:
#                     confidence = max(0.7, similarity + 0.1)
#                 else:
#                     confidence = min(0.4, similarity)
            
#             reason_patterns = [r"ç†ç”±ï¼š(.*?)(?=\n|$)", r"ç†ç”±:(.*?)(?=\n|$)", r"ç†ç”±: (.*?)(?=\n|$)"]
#             reason = ""
            
#             for pattern in reason_patterns:
#                 match_obj = re.search(pattern, response, re.DOTALL)
#                 if match_obj:
#                     reason = match_obj.group(1).strip()
#                     break
            
#             if not reason:
#                 reason = response[:100] + "..."
            
#             return {
#                 "match": match,
#                 "confidence": confidence,
#                 "reason": reason,
#                 "llm_response": response
#             }
            
#         except Exception as e:
#             return {
#                 "match": similarity > 0.6,
#                 "confidence": similarity,
#                 "reason": f"è§£æå¤±è´¥: {e}ï¼ŒåŸºäºç›¸ä¼¼åº¦åˆ¤æ–­",
#                 "parse_error": True
#             }
    
#     def _generate_fallback_result(self, source_field: Dict, target_field: Dict, 
#                                  similarity: float, error_msg: str) -> Dict:
#         """ç”Ÿæˆå›é€€ç»“æœ"""
#         s_name = source_field.get("name", "").lower()
#         t_name = target_field.get("name", "").lower()
        
#         if s_name == t_name:
#             match = True
#             confidence = 0.95
#         elif similarity >= 0.8:
#             match = True
#             confidence = 0.8
#         elif similarity >= 0.6:
#             match = True
#             confidence = 0.7
#         elif similarity >= 0.4 and self.enable_aggressive_mode:
#             match = True
#             confidence = 0.6
#         else:
#             match = False
#             confidence = similarity
        
#         reason = f"APIè°ƒç”¨å¤±è´¥ï¼ŒåŸºäºç›¸ä¼¼åº¦{similarity:.2f}çš„æ™ºèƒ½å›é€€"
        
#         return {
#             "match": match,
#             "confidence": confidence,
#             "reason": reason,
#             "fallback": True
#         }
    
#     def batch_process_candidates(self, candidate_pairs: List[Dict], 
#                                 source_schemas: Dict[str, Dict],
#                                 target_schemas: Dict[str, Dict],
#                                 max_candidates: int = None) -> List[Dict]:
#         """æ‰¹é‡å¤„ç†å€™é€‰åŒ¹é…å¯¹"""
        
#         if max_candidates and len(candidate_pairs) > max_candidates:
#             print(f"å€™é€‰å¯¹æ•°é‡({len(candidate_pairs)})è¶…è¿‡é™åˆ¶({max_candidates})ï¼Œåªå¤„ç†å‰{max_candidates}ä¸ª")
#             candidate_pairs = candidate_pairs[:max_candidates]
        
#         results = []
        
#         print(f"å¼€å§‹å¤„ç† {len(candidate_pairs)} å¯¹å€™é€‰åŒ¹é…...")
        
#         # åˆ†å±‚å¤„ç†
#         high_sim = [c for c in candidate_pairs if c["similarity"] >= 0.7]
#         medium_sim = [c for c in candidate_pairs if 0.4 <= c["similarity"] < 0.7]
#         low_sim = [c for c in candidate_pairs if c["similarity"] < 0.4]
        
#         print(f"é«˜ç›¸ä¼¼åº¦: {len(high_sim)}, ä¸­ç­‰ç›¸ä¼¼åº¦: {len(medium_sim)}, ä½ç›¸ä¼¼åº¦: {len(low_sim)}")
        
#         for candidates, desc in [(high_sim, "é«˜ç›¸ä¼¼åº¦"), (medium_sim, "ä¸­ç­‰ç›¸ä¼¼åº¦"), (low_sim, "ä½ç›¸ä¼¼åº¦")]:
#             if not candidates:
#                 continue
                
#             print(f"å¤„ç†{desc}å€™é€‰å¯¹...")
            
#             for candidate in tqdm(candidates, desc=f"å¤„ç†{desc}"):
#                 try:
#                     source_schema = source_schemas[candidate["source_table"]]
#                     source_field = next(f for f in source_schema["fields"] if f["name"] == candidate["source_field"])
                    
#                     target_schema = target_schemas[candidate["target_table"]]
#                     target_field = next(f for f in target_schema["fields"] if f["name"] == candidate["target_field"])
                    
#                     result = self.match_field_pair(source_field, target_field, candidate["similarity"])
                    
#                     result["source_table"] = candidate["source_table"]
#                     result["source_field"] = candidate["source_field"]
#                     result["target_table"] = candidate["target_table"]
#                     result["target_field"] = candidate["target_field"]
                    
#                     results.append(result)
                    
#                     # APIè°ƒç”¨é—´éš”
#                     if not result.get("fallback", False):
#                         time.sleep(1)
                        
#                 except Exception as e:
#                     print(f"å¤„ç†å¤±è´¥: {candidate}, é”™è¯¯: {e}")
        
#         if self.cache_enabled:
#             self._save_cache()
        
#         print(f"\n=== APIè°ƒç”¨ç»Ÿè®¡ ===")
#         print(f"æˆåŠŸè°ƒç”¨: {self.success_count}")
#         print(f"å¤±è´¥è°ƒç”¨: {self.fail_count}")
#         print(f"ç¼“å­˜å‘½ä¸­: {self.cache_hit_count}")
        
#         return results
    
#     def _save_cache(self):
#         """ä¿å­˜ç¼“å­˜"""
#         try:
#             with open(self._cache_path, "w", encoding="utf-8") as f:
#                 json.dump(self._cache, f, ensure_ascii=False, indent=2)
#         except Exception as e:
#             print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")


# def interactive_parameter_setup():
#     """äº¤äº’å¼å‚æ•°è®¾ç½®"""
#     print("\n" + "="*60)
#     print("                 äº¤äº’å¼å‚æ•°è®¾ç½®")
#     print("="*60)
    
#     params = {}
    
#     # æ•°æ®æ–‡ä»¶é€‰æ‹©
#     print("\n1. æ•°æ®æ–‡ä»¶é€‰æ‹©")
#     source_options = [
#         "data/æºæ•°æ®å­—å…¸.xlsx",
#         "data/è‡ªå®šä¹‰æºæ–‡ä»¶.xlsx"
#     ]
#     target_options = [
#         "data/é¡¹ç›®åŒ¹é…å­—å…¸.xlsx", 
#         "data/é¡¹ç›®åŒ¹é…å­—å…¸_åˆ—ç±»å‹æ³¨é‡Š.xlsx",
#         "data/è‡ªå®šä¹‰ç›®æ ‡æ–‡ä»¶.xlsx"
#     ]
    
#     print("æºæ–‡ä»¶é€‰é¡¹:")
#     for i, option in enumerate(source_options):
#         exists = "âœ…" if os.path.exists(option) else "âŒ"
#         print(f"  {i+1}. {option} {exists}")
    
#     while True:
#         try:
#             choice = int(input("é€‰æ‹©æºæ–‡ä»¶ (1-2): ")) - 1
#             if 0 <= choice < len(source_options):
#                 params["source_file"] = source_options[choice]
#                 break
#         except ValueError:
#             pass
#         print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
    
#     print("\nç›®æ ‡æ–‡ä»¶é€‰é¡¹:")
#     for i, option in enumerate(target_options):
#         exists = "âœ…" if os.path.exists(option) else "âŒ"
#         print(f"  {i+1}. {option} {exists}")
    
#     while True:
#         try:
#             choice = int(input("é€‰æ‹©ç›®æ ‡æ–‡ä»¶ (1-3): ")) - 1
#             if 0 <= choice < len(target_options):
#                 params["target_file"] = target_options[choice]
#                 break
#         except ValueError:
#             pass
#         print("æ— æ•ˆé€‰æ‹©ï¼Œè¯·é‡æ–°è¾“å…¥")
    
#     # é˜ˆå€¼è®¾ç½®
#     print("\n2. é˜ˆå€¼å‚æ•°è®¾ç½®")
#     print("å½“å‰é»˜è®¤å€¼ï¼š")
#     print("  ç›¸ä¼¼åº¦é˜ˆå€¼: 0.2 (åˆç­›å€™é€‰å¯¹)")
#     print("  é«˜ç½®ä¿¡åº¦é˜ˆå€¼: 0.8")
#     print("  ä¸­ç­‰ç½®ä¿¡åº¦é˜ˆå€¼: 0.6")
#     print("  ä½ç½®ä¿¡åº¦é˜ˆå€¼: 0.4")
    
#     if input("\næ˜¯å¦ä¿®æ”¹é˜ˆå€¼è®¾ç½®? (y/n): ").lower() == 'y':
#         params["similarity_threshold"] = float(input("ç›¸ä¼¼åº¦é˜ˆå€¼ (0-1, å»ºè®®0.2): ") or "0.2")
#         params["high_confidence"] = float(input("é«˜ç½®ä¿¡åº¦é˜ˆå€¼ (0-1, å»ºè®®0.8): ") or "0.8")
#         params["medium_confidence"] = float(input("ä¸­ç­‰ç½®ä¿¡åº¦é˜ˆå€¼ (0-1, å»ºè®®0.6): ") or "0.6")
#         params["low_confidence"] = float(input("ä½ç½®ä¿¡åº¦é˜ˆå€¼ (0-1, å»ºè®®0.4): ") or "0.4")
    
#     # å¤„ç†æ•°é‡è®¾ç½®
#     print("\n3. å¤„ç†æ•°é‡è®¾ç½®")
#     params["max_table_pairs"] = int(input("æœ€å¤§è¡¨å¯¹æ•°é‡ (å»ºè®®20): ") or "20")
#     params["max_llm_candidates"] = int(input("æœ€å¤§LLMå¤„ç†å€™é€‰å¯¹æ•°é‡ (å»ºè®®50): ") or "50")
    
#     # åŒ¹é…ç­–ç•¥è®¾ç½®
#     print("\n4. åŒ¹é…ç­–ç•¥è®¾ç½®")
#     params["aggressive_mode"] = input("å¯ç”¨ç§¯æåŒ¹é…æ¨¡å¼? (y/n, å»ºè®®y): ").lower() == 'y'
#     params["batch_size"] = int(input("æ‰¹å¤„ç†å¤§å° (å»ºè®®3): ") or "3")
    
#     return params


# def calculate_table_similarity(source_schema, target_schema):
#     """è®¡ç®—è¡¨çº§åˆ«ç›¸ä¼¼åº¦"""
#     source_name = source_schema.table_name.lower()
#     target_name = target_schema.table_name.lower()
    
#     import difflib
#     name_sim = difflib.SequenceMatcher(None, source_name, target_name).ratio()
    
#     source_desc = source_schema.table_desc.lower() if source_schema.table_desc else ""
#     target_desc = target_schema.table_desc.lower() if target_schema.table_desc else ""
    
#     if source_desc and target_desc:
#         desc_sim = difflib.SequenceMatcher(None, source_desc, target_desc).ratio()
#     else:
#         desc_sim = 0
    
#     return 0.7 * name_sim + 0.3 * desc_sim


# def filter_table_pairs(source_schemas, target_schemas, table_threshold=0.2, max_pairs=20):
#     """ç­›é€‰è¡¨å¯¹"""
#     table_similarities = []
    
#     for source_schema in source_schemas:
#         for target_schema in target_schemas:
#             similarity = calculate_table_similarity(source_schema, target_schema)
#             if similarity >= table_threshold:
#                 table_similarities.append((source_schema, target_schema, similarity))
    
#     table_similarities.sort(key=lambda x: x[2], reverse=True)
    
#     if len(table_similarities) > max_pairs:
#         table_similarities = table_similarities[:max_pairs]
    
#     selected_pairs = [(s, t) for s, t, _ in table_similarities]
    
#     print(f"ç­›é€‰å‡º {len(selected_pairs)} å¯¹æ½œåœ¨åŒ¹é…çš„è¡¨")
#     for i, (source, target, sim) in enumerate(table_similarities[:10]):  # æ˜¾ç¤ºå‰10ä¸ª
#         print(f"  {i+1}. {source.table_name} <-> {target.table_name} (ç›¸ä¼¼åº¦: {sim:.3f})")
    
#     if len(table_similarities) > 10:
#         print(f"  ... è¿˜æœ‰ {len(table_similarities) - 10} å¯¹è¡¨")
    
#     return selected_pairs


# def classify_matches_by_confidence(results: List[Dict], matcher) -> Dict[str, List[Dict]]:
#     """æ ¹æ®ç½®ä¿¡åº¦åˆ†ç±»åŒ¹é…ç»“æœ"""
#     classified = {
#         "high_confidence": [],
#         "medium_confidence": [],
#         "low_confidence": [],
#         "potential_matches": []
#     }
    
#     for result in results:
#         if not result.get("match", False):
#             continue
            
#         confidence = result.get("confidence", 0)
        
#         if confidence >= matcher.high_confidence_threshold:
#             classified["high_confidence"].append(result)
#         elif confidence >= matcher.medium_confidence_threshold:
#             classified["medium_confidence"].append(result)
#         elif confidence >= matcher.low_confidence_threshold:
#             classified["low_confidence"].append(result)
#         else:
#             classified["potential_matches"].append(result)
    
#     return classified


# def main():
#     """ä¸»å‡½æ•°"""
#     parser = argparse.ArgumentParser(description='äº¤äº’å¼å¢å¼ºSchemaåŒ¹é…')
#     parser.add_argument('--config', type=str, default='config/config_enhanced.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
#     parser.add_argument('--output', type=str, default='output', help='è¾“å‡ºç›®å½•')
#     parser.add_argument('--auto', action='store_true', help='ä½¿ç”¨é»˜è®¤å‚æ•°ï¼Œè·³è¿‡äº¤äº’å¼è®¾ç½®')
#     args = parser.parse_args()
    
#     print("=== äº¤äº’å¼å¢å¼ºSchemaåŒ¹é…ç³»ç»Ÿ ===")
#     print("æ”¯æŒåŠ¨æ€å‚æ•°è°ƒæ•´å’Œå®æ—¶é˜ˆå€¼ä¿®æ”¹")
    
#     # æ£€æŸ¥é…ç½®æ–‡ä»¶
#     if not os.path.exists(args.config):
#         print(f"é”™è¯¯: é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
#         sys.exit(1)
    
#     # äº¤äº’å¼å‚æ•°è®¾ç½®æˆ–ä½¿ç”¨é»˜è®¤å€¼
#     if args.auto:
#         print("ä½¿ç”¨é»˜è®¤å‚æ•°è¿è¡Œ...")
#         params = {
#             "source_file": "data/æºæ•°æ®å­—å…¸.xlsx",
#             "target_file": "data/é¡¹ç›®åŒ¹é…å­—å…¸_åˆ—ç±»å‹æ³¨é‡Š.xlsx",
#             "max_table_pairs": 20,
#             "max_llm_candidates": 50,
#             "aggressive_mode": True,
#             "batch_size": 3
#         }
#     else:
#         params = interactive_parameter_setup()
    
#     # æ£€æŸ¥æ•°æ®æ–‡ä»¶
#     for file_key in ["source_file", "target_file"]:
#         if not os.path.exists(params[file_key]):
#             print(f"é”™è¯¯: {file_key}ä¸å­˜åœ¨: {params[file_key]}")
#             sys.exit(1)
    
#     # åŠ è½½é…ç½®
#     with open(args.config, "r", encoding="utf-8") as f:
#         config = yaml.safe_load(f)
    
#     print(f"\nä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
#     print(f"æºæ–‡ä»¶: {params['source_file']}")
#     print(f"ç›®æ ‡æ–‡ä»¶: {params['target_file']}")
    
#     # 1. æ•°æ®åŠ è½½
#     print("\n" + "="*50)
#     print("1. æ•°æ®åŠ è½½")
#     print("="*50)
#     start_time = time.time()
#     data_loader = DataLoader()
    
#     try:
#         source_schemas = data_loader.load_excel_dictionary(params["source_file"])
#         target_schemas = data_loader.load_excel_dictionary(params["target_file"])
        
#         print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
#         print(f"æºè¡¨æ•°é‡: {len(source_schemas)}, ç›®æ ‡è¡¨æ•°é‡: {len(target_schemas)}")
        
#     except Exception as e:
#         print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
#         return
    
#     # 2. å…ƒæ•°æ®é¢„å¤„ç†
#     print("\n" + "="*50)
#     print("2. å…ƒæ•°æ®é¢„å¤„ç†")
#     print("="*50)
#     start_time = time.time()
    
#     preprocessor = MetadataPreprocessor(
#         enable_pinyin=config["chinese"]["enable_pinyin"],
#         enable_abbreviation=config["chinese"]["enable_abbreviation"]
#     )
    
#     processed_source_schemas = {}
#     for schema in source_schemas:
#         processed = preprocessor.preprocess_schema(schema)
#         processed_source_schemas[schema.table_name] = processed
    
#     processed_target_schemas = {}
#     for schema in target_schemas:
#         processed = preprocessor.preprocess_schema(schema)
#         processed_target_schemas[schema.table_name] = processed
    
#     print(f"âœ… é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
    
#     # 3. è¡¨å¯¹ç­›é€‰
#     print("\n" + "="*50)
#     print("3. è¡¨å¯¹ç­›é€‰")
#     print("="*50)
    
#     table_pairs = filter_table_pairs(
#         source_schemas, 
#         target_schemas, 
#         max_pairs=params["max_table_pairs"]
#     )
    
#     # 4. å¢å¼ºç›¸ä¼¼åº¦è®¡ç®—
#     print("\n" + "="*50)
#     print("4. å¢å¼ºç›¸ä¼¼åº¦è®¡ç®—")
#     print("="*50)
#     start_time = time.time()
    
#     similarity_calculator = EnhancedSimilarityCalculator(
#         char_weight=config["similarity"]["char_weight"],
#         semantic_weight=config["similarity"]["semantic_weight"],
#         struct_weight=config["similarity"]["struct_weight"],
#         pinyin_boost=config["similarity"]["pinyin_boost"]
#     )
    
#     all_candidates = []
    
#     for source_schema, target_schema in table_pairs:
#         print(f"è®¡ç®—è¡¨ {source_schema.table_name} å’Œ {target_schema.table_name} çš„ç›¸ä¼¼åº¦...")
        
#         source_processed = processed_source_schemas[source_schema.table_name]
#         target_processed = processed_target_schemas[target_schema.table_name]
        
#         matrix = similarity_calculator.calculate_similarity_matrix(
#             source_processed["fields"],
#             target_processed["fields"]
#         )
        
#         # ä½¿ç”¨åŠ¨æ€é˜ˆå€¼
#         similarity_threshold = params.get("similarity_threshold", config["thresholds"]["similarity_threshold"])
        
#         for i, s_field in enumerate(source_processed["fields"]):
#             for j, t_field in enumerate(target_processed["fields"]):
#                 sim = matrix[i, j]
#                 if sim >= similarity_threshold:
#                     all_candidates.append({
#                         "source_table": source_schema.table_name,
#                         "source_field": s_field["name"],
#                         "target_table": target_schema.table_name,
#                         "target_field": t_field["name"],
#                         "similarity": float(sim)
#                     })
    
#     all_candidates.sort(key=lambda x: x["similarity"], reverse=True)
    
#     print(f"âœ… ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
#     print(f"æ‰¾åˆ° {len(all_candidates)} å¯¹å€™é€‰å­—æ®µåŒ¹é…ï¼ˆé˜ˆå€¼: {similarity_threshold}ï¼‰")
    
#     # æ˜¾ç¤ºç›¸ä¼¼åº¦åˆ†å¸ƒ
#     high_sim = len([c for c in all_candidates if c["similarity"] >= 0.7])
#     medium_sim = len([c for c in all_candidates if 0.4 <= c["similarity"] < 0.7])
#     low_sim = len([c for c in all_candidates if c["similarity"] < 0.4])
#     print(f"  é«˜ç›¸ä¼¼åº¦(â‰¥0.7): {high_sim}")
#     print(f"  ä¸­ç­‰ç›¸ä¼¼åº¦(0.4-0.7): {medium_sim}")
#     print(f"  ä½ç›¸ä¼¼åº¦(<0.4): {low_sim}")
    
#     # 5. åº”ç”¨åŒ¹é…è§„åˆ™
#     print("\n" + "="*50)
#     print("5. åº”ç”¨åŒ¹é…è§„åˆ™")
#     print("="*50)
    
#     candidate_filter = CandidateFilter(similarity_threshold=similarity_threshold)
#     filtered_candidates = candidate_filter.apply_matching_rules(all_candidates)
    
#     print(f"âœ… åº”ç”¨è§„åˆ™åä¿ç•™ {len(filtered_candidates)} å¯¹å€™é€‰åŒ¹é…")
    
#     # 6. åˆå§‹åŒ–äº¤äº’å¼LLMåŒ¹é…å™¨
#     print("\n" + "="*50)
#     print("6. åˆå§‹åŒ–äº¤äº’å¼LLMåŒ¹é…å™¨")
#     print("="*50)
    
#     llm_matcher = InteractiveEnhancedLLMMatcher(config_path=args.config)
    
#     # åº”ç”¨ç”¨æˆ·è®¾ç½®çš„å‚æ•°
#     if "similarity_threshold" in params:
#         llm_matcher.update_thresholds(similarity_threshold=params["similarity_threshold"])
#     if "high_confidence" in params:
#         llm_matcher.update_thresholds(high_conf=params["high_confidence"])
#     if "medium_confidence" in params:
#         llm_matcher.update_thresholds(medium_conf=params["medium_confidence"])
#     if "low_confidence" in params:
#         llm_matcher.update_thresholds(low_conf=params["low_confidence"])
    
#     llm_matcher.update_matching_settings(
#         batch_size=params["batch_size"],
#         aggressive_mode=params["aggressive_mode"]
#     )
    
#     # 7. LLMåŒ¹é…
#     print("\n" + "="*50)
#     print("7. äº¤äº’å¼LLMåŒ¹é…")
#     print("="*50)
#     start_time = time.time()
    
#     matching_results = llm_matcher.batch_process_candidates(
#         filtered_candidates,
#         processed_source_schemas,
#         processed_target_schemas,
#         max_candidates=params["max_llm_candidates"]
#     )
    
#     print(f"âœ… LLMåŒ¹é…å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
    
#     # 8. ç»“æœåˆ†ç±»å’Œå¤„ç†
#     print("\n" + "="*50)
#     print("8. ç»“æœåˆ†ç±»å’Œå¤„ç†")
#     print("="*50)
    
#     classified_matches = classify_matches_by_confidence(matching_results, llm_matcher)
    
#     result_processor = ResultProcessor(
#         confidence_threshold=llm_matcher.low_confidence_threshold
#     )
    
#     statistics = result_processor.calculate_matching_statistics(
#         matching_results,
#         processed_source_schemas,
#         processed_target_schemas,
#         all_candidates
#     )
    
#     high_confidence_matches = result_processor.process_matching_results(
#         classified_matches["high_confidence"],
#         processed_source_schemas,
#         processed_target_schemas
#     )
    
#     # 9. ä¿å­˜ç»“æœ
#     print("\n" + "="*50)
#     print("9. ä¿å­˜ç»“æœ")
#     print("="*50)
    
#     os.makedirs(args.output, exist_ok=True)
    
#     high_conf_files = result_processor.save_results(high_confidence_matches, statistics, args.output)
    
#     timestamp = pd.Timestamp.now().strftime("%Y%m%d%H%M%S")
#     all_matches_file = os.path.join(args.output, f"interactive_enhanced_matches_{timestamp}.xlsx")
    
#     with pd.ExcelWriter(all_matches_file, engine='openpyxl') as writer:
#         for category, matches in classified_matches.items():
#             if matches:
#                 df = pd.DataFrame(matches)
#                 columns = {
#                     "source_table": "æºè¡¨å",
#                     "source_field": "æºå­—æ®µå", 
#                     "target_table": "ç›®æ ‡è¡¨å",
#                     "target_field": "ç›®æ ‡å­—æ®µå",
#                     "confidence": "åŒ¹é…ç½®ä¿¡åº¦",
#                     "similarity": "ç‰¹å¾ç›¸ä¼¼åº¦",
#                     "reason": "åŒ¹é…ç†ç”±"
#                 }
#                 existing_columns = {k: v for k, v in columns.items() if k in df.columns}
#                 df_output = df[list(existing_columns.keys())].copy()
#                 df_output.rename(columns=existing_columns, inplace=True)
                
#                 sheet_name = {
#                     "high_confidence": "é«˜ç½®ä¿¡åº¦åŒ¹é…",
#                     "medium_confidence": "ä¸­ç­‰ç½®ä¿¡åº¦åŒ¹é…", 
#                     "low_confidence": "ä½ç½®ä¿¡åº¦åŒ¹é…",
#                     "potential_matches": "æ½œåœ¨åŒ¹é…"
#                 }[category]
                
#                 df_output.to_excel(writer, sheet_name=sheet_name, index=False)
    
#     # 10. è¾“å‡ºç»“æœæ€»ç»“
#     print("\n" + "="*60)
#     print("                  äº¤äº’å¼åŒ¹é…ç»“æœæ€»ç»“")
#     print("="*60)
    
#     total_matches = sum(len(matches) for matches in classified_matches.values())
    
#     print(f"\nã€ä½¿ç”¨å‚æ•°ã€‘")
#     print(f"  ç›¸ä¼¼åº¦é˜ˆå€¼: {llm_matcher.similarity_threshold}")
#     print(f"  é«˜ç½®ä¿¡åº¦é˜ˆå€¼: {llm_matcher.high_confidence_threshold}")
#     print(f"  ä¸­ç­‰ç½®ä¿¡åº¦é˜ˆå€¼: {llm_matcher.medium_confidence_threshold}")
#     print(f"  ä½ç½®ä¿¡åº¦é˜ˆå€¼: {llm_matcher.low_confidence_threshold}")
#     print(f"  ç§¯æåŒ¹é…æ¨¡å¼: {'å¯ç”¨' if llm_matcher.enable_aggressive_mode else 'ç¦ç”¨'}")
    
#     print(f"\nã€åŒ¹é…ç»“æœã€‘")
#     print(f"  æ€»åŒ¹é…æ•°é‡: {total_matches}")
#     print(f"  é«˜ç½®ä¿¡åº¦åŒ¹é…: {len(classified_matches['high_confidence'])}")
#     print(f"  ä¸­ç­‰ç½®ä¿¡åº¦åŒ¹é…: {len(classified_matches['medium_confidence'])}")
#     print(f"  ä½ç½®ä¿¡åº¦åŒ¹é…: {len(classified_matches['low_confidence'])}")
#     print(f"  æ½œåœ¨åŒ¹é…: {len(classified_matches['potential_matches'])}")
    
#     print(f"\nã€è¾“å‡ºæ–‡ä»¶ã€‘")
#     print(f"  é«˜ç½®ä¿¡åº¦åŒ¹é…: {high_conf_files['excel']}")
#     print(f"  æ‰€æœ‰åˆ†å±‚åŒ¹é…ç»“æœ: {all_matches_file}")
#     print(f"  ç»Ÿè®¡ä¿¡æ¯: {high_conf_files['statistics']}")
    
#     # æ˜¾ç¤ºåŒ¹é…ç»“æœç¤ºä¾‹
#     if total_matches > 0:
#         print(f"\nã€åŒ¹é…ç»“æœç¤ºä¾‹ã€‘")
#         for category, matches in classified_matches.items():
#             if matches:
#                 category_name = {
#                     "high_confidence": "é«˜ç½®ä¿¡åº¦åŒ¹é…",
#                     "medium_confidence": "ä¸­ç­‰ç½®ä¿¡åº¦åŒ¹é…",
#                     "low_confidence": "ä½ç½®ä¿¡åº¦åŒ¹é…", 
#                     "potential_matches": "æ½œåœ¨åŒ¹é…"
#                 }[category]
                
#                 print(f"\n{category_name} ({len(matches)}ä¸ª):")
#                 for i, result in enumerate(matches[:2]):
#                     fallback_mark = " [å›é€€]" if result.get("fallback", False) else ""
#                     print(f"  {i+1}. {result['source_table']}.{result['source_field']} <-> "
#                           f"{result['target_table']}.{result['target_field']}{fallback_mark}")
#                     print(f"     ç½®ä¿¡åº¦: {result['confidence']:.2f}, ç›¸ä¼¼åº¦: {result.get('similarity', 0):.2f}")
                
#                 if len(matches) > 2:
#                     print(f"     ... è¿˜æœ‰ {len(matches) - 2} ä¸ªåŒ¹é…")
#     else:
#         print(f"\næœªæ‰¾åˆ°åŒ¹é…ç»“æœ")
    
#     print("\nğŸ‰ äº¤äº’å¼å¢å¼ºåŒ¹é…å®éªŒå®Œæˆï¼")


# if __name__ == "__main__":
#     main()
"""
è¶…ç¨³å®šSchemaåŒ¹é…è„šæœ¬ - è§£å†³APIé¢‘ç‡é™åˆ¶é—®é¢˜
"""
import os
import sys
import yaml
import pandas as pd
import numpy as np
from typing import Dict, List, Tuple, Any, Optional
import time
import argparse
import json
import re
import requests
from tqdm import tqdm
import random
import threading

from src.data.data_loader import DataLoader, SchemaMetadata
from src.data.data_preprocessor import MetadataPreprocessor
from src.features.enhanced_similarity_calculator import EnhancedSimilarityCalculator
from src.matching.candidate_filter import CandidateFilter
from src.matching.result_processor import ResultProcessor


class UltraStableLLMMatcher:
    """è¶…ç¨³å®šLLMåŒ¹é…å™¨ - å…·æœ‰æ™ºèƒ½é‡è¯•å’Œé€Ÿç‡æ§åˆ¶"""
    
    def __init__(self, config_path: str = "config/config_enhanced.yaml"):
        print(f"åˆå§‹åŒ–è¶…ç¨³å®šLLMåŒ¹é…å™¨...")
        
        with open(config_path, "r", encoding="utf-8") as f:
            self.config = yaml.safe_load(f)
        
        self.api_key = self.config["openai"]["api_key"]
        self.api_base_url = self.config["openai"]["api_base_url"]
        self.model = self.config["openai"]["model"]
        self.temperature = self.config["openai"]["temperature"]
        self.max_tokens = self.config["openai"]["max_tokens"]
        
        # æ™ºèƒ½é€Ÿç‡æ§åˆ¶å‚æ•°
        self.base_delay = 2.0  # åŸºç¡€å»¶è¿Ÿï¼ˆç§’ï¼‰
        self.max_delay = 30.0  # æœ€å¤§å»¶è¿Ÿ
        self.current_delay = self.base_delay
        self.success_count_for_speedup = 5  # è¿ç»­æˆåŠŸæ¬¡æ•°ååŠ é€Ÿ
        self.consecutive_successes = 0
        
        # é‡è¯•æœºåˆ¶å‚æ•°
        self.max_retries = 3
        self.retry_delays = [2, 5, 10]  # é‡è¯•é—´éš”
        
        # ç¼“å­˜
        self.cache_enabled = self.config["system"]["cache_enabled"]
        self._cache = {}
        self._cache_path = "cache/ultra_stable_responses.json"
        if self.cache_enabled:
            os.makedirs(os.path.dirname(self._cache_path), exist_ok=True)
            if os.path.exists(self._cache_path):
                with open(self._cache_path, "r", encoding="utf-8") as f:
                    self._cache = json.load(f)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.total_calls = 0
        self.success_count = 0
        self.fail_count = 0
        self.cache_hit_count = 0
        self.retry_count = 0
        
        # çº¿ç¨‹é”ï¼Œç¡®ä¿APIè°ƒç”¨ä¸²è¡Œ
        self.api_lock = threading.Lock()
        
        print(f"âœ… è¶…ç¨³å®šåŒ¹é…å™¨åˆå§‹åŒ–å®Œæˆ")
        print(f"APIé…ç½®: {self.api_base_url}")
        print(f"åŸºç¡€å»¶è¿Ÿ: {self.base_delay}ç§’")
        print(f"æœ€å¤§é‡è¯•æ¬¡æ•°: {self.max_retries}")
    
    def _call_api_with_smart_retry(self, prompt: str, field_info: str = "") -> str:
        """æ™ºèƒ½é‡è¯•çš„APIè°ƒç”¨"""
        with self.api_lock:  # ç¡®ä¿ä¸²è¡Œè°ƒç”¨
            self.total_calls += 1
            
            for attempt in range(self.max_retries + 1):
                try:
                    if attempt > 0:
                        self.retry_count += 1
                        retry_delay = self.retry_delays[min(attempt - 1, len(self.retry_delays) - 1)]
                        # æ·»åŠ éšæœºå› å­ï¼Œé¿å…é›·ç¾¤æ•ˆåº”
                        actual_delay = retry_delay + random.uniform(0, 2)
                        print(f"  ğŸ”„ ç¬¬{attempt}æ¬¡é‡è¯• {field_info}ï¼Œç­‰å¾…{actual_delay:.1f}ç§’...")
                        time.sleep(actual_delay)
                    
                    # è°ƒç”¨API
                    response = self._make_api_request(prompt)
                    
                    if not response.startswith("é”™è¯¯ï¼š"):
                        # æˆåŠŸï¼
                        self.success_count += 1
                        self.consecutive_successes += 1
                        
                        # åŠ¨æ€è°ƒæ•´å»¶è¿Ÿï¼šè¿ç»­æˆåŠŸæ—¶é€‚åº¦åŠ é€Ÿ
                        if self.consecutive_successes >= self.success_count_for_speedup:
                            self.current_delay = max(1.0, self.current_delay * 0.8)
                            self.consecutive_successes = 0
                            print(f"  âš¡ è°ƒæ•´å»¶è¿Ÿä¸º: {self.current_delay:.1f}ç§’")
                        
                        # æˆåŠŸè°ƒç”¨åçš„æ ‡å‡†å»¶è¿Ÿ
                        time.sleep(self.current_delay)
                        return response
                    else:
                        # å¤±è´¥ï¼Œé‡ç½®è¿ç»­æˆåŠŸè®¡æ•°
                        self.consecutive_successes = 0
                        
                        # æ£€æŸ¥æ˜¯å¦æ˜¯é¢‘ç‡é™åˆ¶é”™è¯¯
                        if "429" in response or "é¢‘ç‡" in response or "rate" in response.lower():
                            # é¢‘ç‡é™åˆ¶ï¼Œå¢åŠ å»¶è¿Ÿ
                            self.current_delay = min(self.max_delay, self.current_delay * 2)
                            print(f"  âš ï¸  æ£€æµ‹åˆ°é¢‘ç‡é™åˆ¶ï¼Œå¢åŠ å»¶è¿Ÿè‡³: {self.current_delay:.1f}ç§’")
                        
                        print(f"  âŒ APIè°ƒç”¨å¤±è´¥ {field_info}: {response[:100]}")
                        
                        if attempt == self.max_retries:
                            # æœ€åä¸€æ¬¡é‡è¯•ä¹Ÿå¤±è´¥
                            self.fail_count += 1
                            return response
                            
                except Exception as e:
                    print(f"  âŒ APIè°ƒç”¨å¼‚å¸¸ {field_info}: {e}")
                    if attempt == self.max_retries:
                        self.fail_count += 1
                        return f"é”™è¯¯ï¼šAPIè°ƒç”¨å¼‚å¸¸ - {str(e)}"
            
            return "é”™è¯¯ï¼šæ‰€æœ‰é‡è¯•éƒ½å¤±è´¥"
    
    def _make_api_request(self, prompt: str) -> str:
        """å®é™…çš„APIè¯·æ±‚"""
        try:
            api_url = f"{self.api_base_url}/chat/completions"
            
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {self.api_key}",
                "User-Agent": "Schema-Matching-Tool/1.0"
            }
            
            data = {
                "model": self.model,
                "messages": [{"role": "user", "content": prompt}],
                "temperature": self.temperature,
                "max_tokens": self.max_tokens
            }
            
            # é€‚å½“å¢åŠ è¶…æ—¶æ—¶é—´
            response = requests.post(api_url, headers=headers, json=data, timeout=45)
            
            if response.status_code == 429:
                return "é”™è¯¯ï¼šAPIé¢‘ç‡é™åˆ¶(429)"
            elif response.status_code == 401:
                return "é”™è¯¯ï¼šAPIè®¤è¯å¤±è´¥(401)"
            elif response.status_code == 403:
                return "é”™è¯¯ï¼šAPIè®¿é—®è¢«ç¦æ­¢(403)"
            elif response.status_code == 500:
                return "é”™è¯¯ï¼šAPIæœåŠ¡å™¨é”™è¯¯(500)"
            elif response.status_code != 200:
                return f"é”™è¯¯ï¼šAPIè°ƒç”¨å¤±è´¥ï¼ŒçŠ¶æ€ç ï¼š{response.status_code}"
            
            try:
                resp_json = response.json()
            except json.JSONDecodeError:
                return "é”™è¯¯ï¼šJSONè§£æå¤±è´¥"
            
            if "choices" in resp_json and len(resp_json["choices"]) > 0:
                choice = resp_json["choices"][0]
                if "message" in choice and "content" in choice["message"]:
                    content = choice["message"]["content"]
                    if content and content.strip():
                        return content
                    else:
                        return "é”™è¯¯ï¼šAPIè¿”å›ç©ºå†…å®¹"
                elif "text" in choice:
                    return choice["text"]
                else:
                    return "é”™è¯¯ï¼šå“åº”æ ¼å¼å¼‚å¸¸"
            else:
                return "é”™è¯¯ï¼šå“åº”ä¸­æ²¡æœ‰choiceså­—æ®µ"
                
        except requests.exceptions.Timeout:
            return "é”™è¯¯ï¼šè¯·æ±‚è¶…æ—¶"
        except requests.exceptions.ConnectionError:
            return "é”™è¯¯ï¼šç½‘ç»œè¿æ¥å¤±è´¥"
        except Exception as e:
            return f"é”™è¯¯ï¼š{str(e)}"
    
    def match_field_pair(self, source_field: Dict, target_field: Dict, similarity: float) -> Dict:
        """åŒ¹é…å•å¯¹å­—æ®µ"""
        field_info = f"{source_field['name']} <-> {target_field['name']}"
        
        # æ£€æŸ¥ç¼“å­˜
        cache_key = f"ultra_{source_field['name']}_{target_field['name']}_{similarity:.3f}"
        if self.cache_enabled and cache_key in self._cache:
            self.cache_hit_count += 1
            print(f"  ğŸ’¾ ç¼“å­˜å‘½ä¸­: {field_info}")
            return self._cache[cache_key]
        
        # åˆ›å»ºç®€åŒ–æç¤º
        prompt = f"""åˆ¤æ–­ä¸¤ä¸ªæ•°æ®åº“å­—æ®µæ˜¯å¦è¯­ä¹‰ç­‰ä»·ï¼š

æºå­—æ®µï¼š{source_field['name']} ({source_field.get('desc', 'æ— æè¿°')})
ç›®æ ‡å­—æ®µï¼š{target_field['name']} ({target_field.get('desc', 'æ— æè¿°')})
ç›¸ä¼¼åº¦ï¼š{similarity:.2f}

åˆ†æè¦ç‚¹ï¼š
1. å­—æ®µåè¯­ä¹‰å…³ç³»ï¼ˆè€ƒè™‘ä¸­è‹±æ–‡å¯¹åº”ã€æ‹¼éŸ³ç¼©å†™ï¼‰
2. ä¸šåŠ¡æ¦‚å¿µæ˜¯å¦ç›¸åŒ
3. æè¿°å†…å®¹åŒ¹é…åº¦

å›ç­”æ ¼å¼ï¼š
åˆ¤æ–­ï¼šæ˜¯/å¦
ç½®ä¿¡åº¦ï¼š0-1æ•°å­—
ç†ç”±ï¼šç®€è¦è¯´æ˜"""
        
        print(f"  ğŸ”„ è°ƒç”¨API: {field_info}")
        
        # æ™ºèƒ½é‡è¯•APIè°ƒç”¨
        response = self._call_api_with_smart_retry(prompt, field_info)
        
        if response.startswith("é”™è¯¯ï¼š"):
            print(f"  ğŸ”„ ä½¿ç”¨æ™ºèƒ½å›é€€: {field_info}")
            result = self._generate_enhanced_fallback_result(source_field, target_field, similarity, response)
        else:
            print(f"  âœ… APIæˆåŠŸ: {field_info}")
            result = self._parse_response(response, similarity)
        
        result["similarity"] = similarity
        
        # ç¼“å­˜ç»“æœ
        if self.cache_enabled:
            self._cache[cache_key] = result
            if len(self._cache) % 5 == 0:
                self._save_cache()
        
        return result
    
    def _parse_response(self, response: str, similarity: float) -> Dict:
        """è§£æLLMå“åº”"""
        try:
            match = False
            if any(indicator in response for indicator in ["åˆ¤æ–­ï¼šæ˜¯", "åˆ¤æ–­:æ˜¯", "åˆ¤æ–­: æ˜¯"]):
                match = True
            
            confidence = 0.0
            patterns = [r"ç½®ä¿¡åº¦ï¼š([0-9.]+)", r"ç½®ä¿¡åº¦:([0-9.]+)", r"ç½®ä¿¡åº¦: ([0-9.]+)"]
            
            for pattern in patterns:
                match_obj = re.search(pattern, response)
                if match_obj:
                    try:
                        confidence = float(match_obj.group(1))
                        break
                    except ValueError:
                        pass
            
            if confidence == 0.0:
                if match:
                    confidence = max(0.7, similarity + 0.1)
                else:
                    confidence = min(0.4, similarity)
            
            reason_patterns = [r"ç†ç”±ï¼š(.*?)(?=\n|$)", r"ç†ç”±:(.*?)(?=\n|$)", r"ç†ç”±: (.*?)(?=\n|$)"]
            reason = ""
            
            for pattern in reason_patterns:
                match_obj = re.search(pattern, response, re.DOTALL)
                if match_obj:
                    reason = match_obj.group(1).strip()
                    break
            
            if not reason:
                reason = response[:100] + "..."
            
            return {
                "match": match,
                "confidence": confidence,
                "reason": reason,
                "llm_response": response
            }
            
        except Exception as e:
            return {
                "match": similarity > 0.6,
                "confidence": similarity,
                "reason": f"è§£æå¤±è´¥: {e}ï¼ŒåŸºäºç›¸ä¼¼åº¦åˆ¤æ–­",
                "parse_error": True
            }
    
    def _generate_enhanced_fallback_result(self, source_field: Dict, target_field: Dict, 
                                          similarity: float, error_msg: str) -> Dict:
        """ç”Ÿæˆå¢å¼ºå›é€€ç»“æœ"""
        s_name = source_field.get("name", "").lower()
        t_name = target_field.get("name", "").lower()
        s_desc = source_field.get("desc", "").lower()
        t_desc = target_field.get("desc", "").lower()
        
        # å¤šå±‚æ¬¡æ™ºèƒ½å›é€€ç­–ç•¥
        
        # 1. å®Œå…¨åŒ¹é…
        if s_name == t_name:
            match = True
            confidence = 0.95
            reason = f"å­—æ®µåå®Œå…¨åŒ¹é… (æ™ºèƒ½å›é€€)"
        
        # 2. é«˜ç›¸ä¼¼åº¦ + ä¸šåŠ¡æ¦‚å¿µåŒ¹é…
        elif similarity >= 0.8:
            # æ£€æŸ¥ä¸šåŠ¡æ¦‚å¿µ
            business_concepts = {
                "id": ["id", "ç¼–å·", "å·ç ", "ä»£ç "],
                "name": ["name", "åç§°", "å§“å", "åå­—"],
                "time": ["time", "æ—¥æœŸ", "æ—¶é—´", "date"],
                "create": ["create", "åˆ›å»º", "æ–°å»º"],
                "update": ["update", "æ›´æ–°", "ä¿®æ”¹"]
            }
            
            concept_match = False
            for concept, keywords in business_concepts.items():
                s_match = any(kw in s_name or kw in s_desc for kw in keywords)
                t_match = any(kw in t_name or kw in t_desc for kw in keywords)
                if s_match and t_match:
                    concept_match = True
                    break
            
            if concept_match:
                match = True
                confidence = min(0.85, similarity + 0.05)
                reason = f"é«˜ç›¸ä¼¼åº¦({similarity:.2f})+ä¸šåŠ¡æ¦‚å¿µåŒ¹é… (æ™ºèƒ½å›é€€)"
            else:
                match = True
                confidence = min(0.8, similarity)
                reason = f"é«˜ç›¸ä¼¼åº¦({similarity:.2f})åŒ¹é… (æ™ºèƒ½å›é€€)"
        
        # 3. ä¸­ç­‰ç›¸ä¼¼åº¦åˆ¤æ–­
        elif similarity >= 0.6:
            match = True
            confidence = min(0.75, similarity + 0.05)
            reason = f"ä¸­ç­‰ç›¸ä¼¼åº¦({similarity:.2f})åŒ¹é… (æ™ºèƒ½å›é€€)"
        
        # 4. åŒ…å«å…³ç³»æ£€æŸ¥
        elif similarity >= 0.4:
            contain_match = False
            if (s_name and t_name and (s_name in t_name or t_name in s_name)) or \
               (s_desc and t_desc and (s_desc in t_desc or t_desc in s_desc)):
                contain_match = True
            
            if contain_match:
                match = True
                confidence = min(0.7, similarity + 0.1)
                reason = f"å­—æ®µåŒ…å«å…³ç³»+ç›¸ä¼¼åº¦({similarity:.2f}) (æ™ºèƒ½å›é€€)"
            else:
                match = False
                confidence = similarity
                reason = f"ç›¸ä¼¼åº¦è¾ƒä½({similarity:.2f})ï¼Œåˆ¤æ–­ä¸ºä¸åŒ¹é… (æ™ºèƒ½å›é€€)"
        
        # 5. ä½ç›¸ä¼¼åº¦
        else:
            match = False
            confidence = similarity
            reason = f"ç›¸ä¼¼åº¦è¿‡ä½({similarity:.2f})ï¼Œåˆ¤æ–­ä¸ºä¸åŒ¹é… (æ™ºèƒ½å›é€€)"
        
        return {
            "match": match,
            "confidence": confidence,
            "reason": reason,
            "fallback": True,
            "api_error": error_msg[:100]
        }
    
    def batch_process_candidates(self, candidate_pairs: List[Dict], 
                                source_schemas: Dict[str, Dict],
                                target_schemas: Dict[str, Dict],
                                max_candidates: int = None) -> List[Dict]:
        """æ‰¹é‡å¤„ç†å€™é€‰åŒ¹é…å¯¹"""
        
        if max_candidates and len(candidate_pairs) > max_candidates:
            print(f"âš ï¸  å€™é€‰å¯¹æ•°é‡({len(candidate_pairs)})è¶…è¿‡é™åˆ¶({max_candidates})ï¼Œåªå¤„ç†å‰{max_candidates}ä¸ª")
            candidate_pairs = candidate_pairs[:max_candidates]
        
        results = []
        
        print(f"ğŸš€ å¼€å§‹è¶…ç¨³å®šæ‰¹é‡å¤„ç† {len(candidate_pairs)} å¯¹å€™é€‰åŒ¹é…...")
        print(f"ğŸ“Š å½“å‰å»¶è¿Ÿè®¾ç½®: {self.current_delay:.1f}ç§’")
        
        # åˆ†å±‚å¤„ç†ï¼Œä¼˜å…ˆå¤„ç†é«˜ç›¸ä¼¼åº¦
        high_sim = [c for c in candidate_pairs if c["similarity"] >= 0.7]
        medium_sim = [c for c in candidate_pairs if 0.4 <= c["similarity"] < 0.7]
        low_sim = [c for c in candidate_pairs if c["similarity"] < 0.4]
        
        print(f"ğŸ“ˆ åˆ†å±‚ç»Ÿè®¡: é«˜ç›¸ä¼¼åº¦({len(high_sim)}) | ä¸­ç­‰ç›¸ä¼¼åº¦({len(medium_sim)}) | ä½ç›¸ä¼¼åº¦({len(low_sim)})")
        
        layer_count = 0
        for candidates, desc in [(high_sim, "é«˜ç›¸ä¼¼åº¦"), (medium_sim, "ä¸­ç­‰ç›¸ä¼¼åº¦"), (low_sim, "ä½ç›¸ä¼¼åº¦")]:
            if not candidates:
                continue
            
            layer_count += 1
            print(f"\nğŸ”„ å¤„ç†ç¬¬{layer_count}å±‚: {desc}å€™é€‰å¯¹ ({len(candidates)}ä¸ª)")
            
            for i, candidate in enumerate(candidates):
                try:
                    print(f"  ğŸ“ è¿›åº¦: {i+1}/{len(candidates)} - {desc}")
                    
                    source_schema = source_schemas[candidate["source_table"]]
                    source_field = next(f for f in source_schema["fields"] if f["name"] == candidate["source_field"])
                    
                    target_schema = target_schemas[candidate["target_table"]]
                    target_field = next(f for f in target_schema["fields"] if f["name"] == candidate["target_field"])
                    
                    result = self.match_field_pair(source_field, target_field, candidate["similarity"])
                    
                    result["source_table"] = candidate["source_table"]
                    result["source_field"] = candidate["source_field"]
                    result["target_table"] = candidate["target_table"]
                    result["target_field"] = candidate["target_field"]
                    
                    results.append(result)
                    
                    # æ˜¾ç¤ºå®æ—¶ç»Ÿè®¡
                    if (i + 1) % 5 == 0:
                        current_success_rate = self.success_count / max(1, self.total_calls) * 100
                        print(f"  ğŸ“Š å½“å‰æˆåŠŸç‡: {current_success_rate:.1f}%, å½“å‰å»¶è¿Ÿ: {self.current_delay:.1f}ç§’")
                        
                except Exception as e:
                    print(f"  âŒ å¤„ç†å¤±è´¥: {candidate}, é”™è¯¯: {e}")
                    error_result = {
                        "source_table": candidate["source_table"],
                        "source_field": candidate["source_field"],
                        "target_table": candidate["target_table"],
                        "target_field": candidate["target_field"],
                        "match": False,
                        "confidence": 0.0,
                        "reason": f"å¤„ç†å¤±è´¥: {e}",
                        "similarity": candidate["similarity"],
                        "error": True
                    }
                    results.append(error_result)
        
        # ä¿å­˜æœ€ç»ˆç¼“å­˜
        if self.cache_enabled:
            self._save_cache()
        
        # æ‰“å°è¯¦ç»†ç»Ÿè®¡
        self._print_detailed_stats()
        
        return results
    
    def _print_detailed_stats(self):
        """æ‰“å°è¯¦ç»†ç»Ÿè®¡ä¿¡æ¯"""
        print(f"\n" + "="*60)
        print("              è¶…ç¨³å®šAPIè°ƒç”¨ç»Ÿè®¡")
        print("="*60)
        
        total_processed = self.total_calls + self.cache_hit_count
        success_rate = self.success_count / max(1, self.total_calls) * 100
        cache_rate = self.cache_hit_count / max(1, total_processed) * 100
        
        print(f"ğŸ“ æ€»APIè°ƒç”¨: {self.total_calls}")
        print(f"âœ… æˆåŠŸè°ƒç”¨: {self.success_count}")
        print(f"âŒ å¤±è´¥è°ƒç”¨: {self.fail_count}")
        print(f"ğŸ”„ é‡è¯•æ¬¡æ•°: {self.retry_count}")
        print(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­: {self.cache_hit_count}")
        print(f"")
        print(f"ğŸ“Š APIæˆåŠŸç‡: {success_rate:.1f}%")
        print(f"ğŸ’¾ ç¼“å­˜å‘½ä¸­ç‡: {cache_rate:.1f}%")
        print(f"âš¡ æœ€ç»ˆå»¶è¿Ÿ: {self.current_delay:.1f}ç§’")
        
        if self.fail_count > 0:
            print(f"")
            print(f"âš ï¸  å¤±è´¥è°ƒç”¨ä½¿ç”¨äº†æ™ºèƒ½å›é€€ç­–ç•¥")
            print(f"ğŸ”„ å»ºè®®ï¼šå¦‚éœ€æé«˜æˆåŠŸç‡ï¼Œå¯å¢åŠ åŸºç¡€å»¶è¿Ÿæ—¶é—´")
        
        print("="*60)
    
    def _save_cache(self):
        """ä¿å­˜ç¼“å­˜"""
        try:
            with open(self._cache_path, "w", encoding="utf-8") as f:
                json.dump(self._cache, f, ensure_ascii=False, indent=2)
        except Exception as e:
            print(f"ä¿å­˜ç¼“å­˜å¤±è´¥: {e}")


def calculate_table_similarity(source_schema, target_schema):
    """è®¡ç®—è¡¨çº§åˆ«ç›¸ä¼¼åº¦"""
    source_name = source_schema.table_name.lower()
    target_name = target_schema.table_name.lower()
    
    import difflib
    name_sim = difflib.SequenceMatcher(None, source_name, target_name).ratio()
    
    source_desc = source_schema.table_desc.lower() if source_schema.table_desc else ""
    target_desc = target_schema.table_desc.lower() if target_schema.table_desc else ""
    
    if source_desc and target_desc:
        desc_sim = difflib.SequenceMatcher(None, source_desc, target_desc).ratio()
    else:
        desc_sim = 0
    
    return 0.7 * name_sim + 0.3 * desc_sim


def filter_table_pairs(source_schemas, target_schemas, table_threshold=0.2, max_pairs=20):
    """ç­›é€‰è¡¨å¯¹"""
    table_similarities = []
    
    for source_schema in source_schemas:
        for target_schema in target_schemas:
            similarity = calculate_table_similarity(source_schema, target_schema)
            if similarity >= table_threshold:
                table_similarities.append((source_schema, target_schema, similarity))
    
    table_similarities.sort(key=lambda x: x[2], reverse=True)
    
    if len(table_similarities) > max_pairs:
        table_similarities = table_similarities[:max_pairs]
    
    selected_pairs = [(s, t) for s, t, _ in table_similarities]
    
    print(f"ç­›é€‰å‡º {len(selected_pairs)} å¯¹æ½œåœ¨åŒ¹é…çš„è¡¨")
    
    return selected_pairs


def classify_matches_by_confidence(results: List[Dict]) -> Dict[str, List[Dict]]:
    """æ ¹æ®ç½®ä¿¡åº¦åˆ†ç±»åŒ¹é…ç»“æœ"""
    classified = {
        "high_confidence": [],
        "medium_confidence": [],
        "low_confidence": [],
        "potential_matches": []
    }
    
    for result in results:
        if not result.get("match", False):
            continue
            
        confidence = result.get("confidence", 0)
        
        if confidence >= 0.8:
            classified["high_confidence"].append(result)
        elif confidence >= 0.6:
            classified["medium_confidence"].append(result)
        elif confidence >= 0.4:
            classified["low_confidence"].append(result)
        else:
            classified["potential_matches"].append(result)
    
    return classified


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='è¶…ç¨³å®šSchemaåŒ¹é…')
    parser.add_argument('--source', type=str, default='data/æºæ•°æ®å­—å…¸.xlsx', help='æºè¡¨æ•°æ®å­—å…¸æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--target', type=str, default='data/é¡¹ç›®åŒ¹é…å­—å…¸_åˆ—ç±»å‹æ³¨é‡Š.xlsx', help='ç›®æ ‡è¡¨æ•°æ®å­—å…¸æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--config', type=str, default='config/config_enhanced.yaml', help='é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default='output', help='è¾“å‡ºç›®å½•')
    parser.add_argument('--max-pairs', type=int, default=15, help='æœ€å¤§è¡¨å¯¹æ•°é‡')
    parser.add_argument('--max-llm', type=int, default=30, help='æœ€å¤§LLMå¤„ç†å€™é€‰å¯¹æ•°é‡')
    parser.add_argument('--base-delay', type=float, default=3.0, help='åŸºç¡€APIè°ƒç”¨å»¶è¿Ÿï¼ˆç§’ï¼‰')
    args = parser.parse_args()
    
    print("ğŸš€ === è¶…ç¨³å®šSchemaåŒ¹é…ç³»ç»Ÿ ===")
    print("âœ¨ å…·æœ‰æ™ºèƒ½é‡è¯•å’ŒåŠ¨æ€é€Ÿç‡æ§åˆ¶")
    print(f"â±ï¸  åŸºç¡€å»¶è¿Ÿ: {args.base_delay}ç§’")
    
    # æ£€æŸ¥æ–‡ä»¶
    for file_path, desc in [(args.config, "é…ç½®æ–‡ä»¶"), (args.source, "æºæ•°æ®æ–‡ä»¶"), (args.target, "ç›®æ ‡æ•°æ®æ–‡ä»¶")]:
        if not os.path.exists(file_path):
            print(f"âŒ é”™è¯¯: {desc}ä¸å­˜åœ¨: {file_path}")
            sys.exit(1)
    
    # åŠ è½½é…ç½®
    with open(args.config, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    
    print(f"ğŸ“„ ä½¿ç”¨é…ç½®æ–‡ä»¶: {args.config}")
    
    # 1. æ•°æ®åŠ è½½
    print(f"\nğŸ“‚ 1. æ•°æ®åŠ è½½...")
    start_time = time.time()
    data_loader = DataLoader()
    
    try:
        source_schemas = data_loader.load_excel_dictionary(args.source)
        target_schemas = data_loader.load_excel_dictionary(args.target)
        
        print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
        print(f"ğŸ“Š æºè¡¨æ•°é‡: {len(source_schemas)}, ç›®æ ‡è¡¨æ•°é‡: {len(target_schemas)}")
        
    except Exception as e:
        print(f"âŒ æ•°æ®åŠ è½½å¤±è´¥: {e}")
        return
    
    # 2. å…ƒæ•°æ®é¢„å¤„ç†
    print(f"\nğŸ”§ 2. å…ƒæ•°æ®é¢„å¤„ç†...")
    start_time = time.time()
    
    preprocessor = MetadataPreprocessor(
        enable_pinyin=config["chinese"]["enable_pinyin"],
        enable_abbreviation=config["chinese"]["enable_abbreviation"]
    )
    
    processed_source_schemas = {}
    for schema in source_schemas:
        processed = preprocessor.preprocess_schema(schema)
        processed_source_schemas[schema.table_name] = processed
    
    processed_target_schemas = {}
    for schema in target_schemas:
        processed = preprocessor.preprocess_schema(schema)
        processed_target_schemas[schema.table_name] = processed
    
    print(f"âœ… é¢„å¤„ç†å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
    
    # 3. è¡¨å¯¹ç­›é€‰
    print(f"\nğŸ“‹ 3. æ™ºèƒ½è¡¨å¯¹ç­›é€‰...")
    table_pairs = filter_table_pairs(
        source_schemas, 
        target_schemas, 
        max_pairs=args.max_pairs
    )
    
    # 4. å¢å¼ºç›¸ä¼¼åº¦è®¡ç®—
    print(f"\nğŸ§® 4. å¢å¼ºç›¸ä¼¼åº¦è®¡ç®—...")
    start_time = time.time()
    
    similarity_calculator = EnhancedSimilarityCalculator(
        char_weight=config["similarity"]["char_weight"],
        semantic_weight=config["similarity"]["semantic_weight"],
        struct_weight=config["similarity"]["struct_weight"],
        pinyin_boost=config["similarity"]["pinyin_boost"]
    )
    
    all_candidates = []
    
    for source_schema, target_schema in table_pairs:
        print(f"  ğŸ”„ è®¡ç®—è¡¨ {source_schema.table_name} <-> {target_schema.table_name}")
        
        source_processed = processed_source_schemas[source_schema.table_name]
        target_processed = processed_target_schemas[target_schema.table_name]
        
        matrix = similarity_calculator.calculate_similarity_matrix(
            source_processed["fields"],
            target_processed["fields"]
        )
        
        similarity_threshold = config["thresholds"]["similarity_threshold"]
        
        for i, s_field in enumerate(source_processed["fields"]):
            for j, t_field in enumerate(target_processed["fields"]):
                sim = matrix[i, j]
                if sim >= similarity_threshold:
                    all_candidates.append({
                        "source_table": source_schema.table_name,
                        "source_field": s_field["name"],
                        "target_table": target_schema.table_name,
                        "target_field": t_field["name"],
                        "similarity": float(sim)
                    })
    
    all_candidates.sort(key=lambda x: x["similarity"], reverse=True)
    
    print(f"âœ… ç›¸ä¼¼åº¦è®¡ç®—å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
    print(f"ğŸ“Š æ‰¾åˆ° {len(all_candidates)} å¯¹å€™é€‰å­—æ®µåŒ¹é…")
    
    # 5. åº”ç”¨åŒ¹é…è§„åˆ™
    print(f"\nğŸ“ 5. åº”ç”¨åŒ¹é…è§„åˆ™...")
    candidate_filter = CandidateFilter(similarity_threshold=config["thresholds"]["similarity_threshold"])
    filtered_candidates = candidate_filter.apply_matching_rules(all_candidates)
    print(f"âœ… åº”ç”¨è§„åˆ™åä¿ç•™ {len(filtered_candidates)} å¯¹å€™é€‰åŒ¹é…")
    
    # é™åˆ¶å¤„ç†æ•°é‡
    if len(filtered_candidates) > args.max_llm:
        print(f"âš ï¸  å€™é€‰åŒ¹é…å¯¹è¾ƒå¤šï¼Œåªå¤„ç†å‰ {args.max_llm} ä¸ª")
        current_candidates = filtered_candidates[:args.max_llm]
    else:
        current_candidates = filtered_candidates
    
    # 6. è¶…ç¨³å®šLLMåŒ¹é…
    print(f"\nğŸ¤– 6. è¶…ç¨³å®šLLMåŒ¹é…...")
    start_time = time.time()
    
    # åˆå§‹åŒ–è¶…ç¨³å®šåŒ¹é…å™¨
    llm_matcher = UltraStableLLMMatcher(config_path=args.config)
    llm_matcher.base_delay = args.base_delay  # åº”ç”¨ç”¨æˆ·è®¾ç½®çš„å»¶è¿Ÿ
    llm_matcher.current_delay = args.base_delay
    
    matching_results = llm_matcher.batch_process_candidates(
        current_candidates,
        processed_source_schemas,
        processed_target_schemas,
        max_candidates=args.max_llm
    )
    
    print(f"âœ… è¶…ç¨³å®šLLMåŒ¹é…å®Œæˆï¼Œè€—æ—¶: {time.time() - start_time:.2f}ç§’")
    
    # 7. ç»“æœåˆ†ç±»å’Œå¤„ç†
    print(f"\nğŸ“Š 7. ç»“æœåˆ†ç±»å’Œå¤„ç†...")
    
    classified_matches = classify_matches_by_confidence(matching_results)
    
    result_processor = ResultProcessor(confidence_threshold=0.4)
    
    statistics = result_processor.calculate_matching_statistics(
        matching_results,
        processed_source_schemas,
        processed_target_schemas,
        all_candidates
    )
    
    high_confidence_matches = result_processor.process_matching_results(
        classified_matches["high_confidence"],
        processed_source_schemas,
        processed_target_schemas
    )
    
    # 8. ä¿å­˜ç»“æœ
    print(f"\nğŸ’¾ 8. ä¿å­˜ç»“æœ...")
    
    os.makedirs(args.output, exist_ok=True)
    
    high_conf_files = result_processor.save_results(high_confidence_matches, statistics, args.output)
    
    timestamp = pd.Timestamp.now().strftime("%Y%m%d%H%M%S")
    all_matches_file = os.path.join(args.output, f"ultra_stable_matches_{timestamp}.xlsx")
    
    with pd.ExcelWriter(all_matches_file, engine='openpyxl') as writer:
        for category, matches in classified_matches.items():
            if matches:
                df = pd.DataFrame(matches)
                columns = {
                    "source_table": "æºè¡¨å",
                    "source_field": "æºå­—æ®µå", 
                    "target_table": "ç›®æ ‡è¡¨å",
                    "target_field": "ç›®æ ‡å­—æ®µå",
                    "confidence": "åŒ¹é…ç½®ä¿¡åº¦",
                    "similarity": "ç‰¹å¾ç›¸ä¼¼åº¦",
                    "reason": "åŒ¹é…ç†ç”±"
                }
                existing_columns = {k: v for k, v in columns.items() if k in df.columns}
                df_output = df[list(existing_columns.keys())].copy()
                df_output.rename(columns=existing_columns, inplace=True)
                
                sheet_name = {
                    "high_confidence": "é«˜ç½®ä¿¡åº¦åŒ¹é…",
                    "medium_confidence": "ä¸­ç­‰ç½®ä¿¡åº¦åŒ¹é…", 
                    "low_confidence": "ä½ç½®ä¿¡åº¦åŒ¹é…",
                    "potential_matches": "æ½œåœ¨åŒ¹é…"
                }[category]
                
                df_output.to_excel(writer, sheet_name=sheet_name, index=False)
    
    # 9. è¾“å‡ºæœ€ç»ˆæ€»ç»“
    print(f"\n" + "="*60)
    print("                  è¶…ç¨³å®šåŒ¹é…æ€»ç»“")
    print("="*60)
    
    total_matches = sum(len(matches) for matches in classified_matches.values())
    api_success_rate = llm_matcher.success_count / max(1, llm_matcher.total_calls) * 100
    
    print(f"\nğŸ¯ ã€åŒ¹é…æˆæœã€‘")
    print(f"  æ€»åŒ¹é…æ•°é‡: {total_matches}")
    print(f"  é«˜ç½®ä¿¡åº¦åŒ¹é…: {len(classified_matches['high_confidence'])}")
    print(f"  ä¸­ç­‰ç½®ä¿¡åº¦åŒ¹é…: {len(classified_matches['medium_confidence'])}")
    print(f"  ä½ç½®ä¿¡åº¦åŒ¹é…: {len(classified_matches['low_confidence'])}")
    print(f"  æ½œåœ¨åŒ¹é…: {len(classified_matches['potential_matches'])}")
    
    print(f"\nğŸ”§ ã€ç³»ç»Ÿæ€§èƒ½ã€‘")
    print(f"  APIæˆåŠŸç‡: {api_success_rate:.1f}%")
    print(f"  æœ€ç»ˆå»¶è¿Ÿ: {llm_matcher.current_delay:.1f}ç§’")
    print(f"  é‡è¯•æ¬¡æ•°: {llm_matcher.retry_count}")
    print(f"  ç¼“å­˜å‘½ä¸­: {llm_matcher.cache_hit_count}")
    
    print(f"\nğŸ“ ã€è¾“å‡ºæ–‡ä»¶ã€‘")
    print(f"  é«˜ç½®ä¿¡åº¦åŒ¹é…: {high_conf_files['excel']}")
    print(f"  æ‰€æœ‰åˆ†å±‚åŒ¹é…ç»“æœ: {all_matches_file}")
    print(f"  ç»Ÿè®¡ä¿¡æ¯: {high_conf_files['statistics']}")
    
    # æ˜¾ç¤ºæˆåŠŸåŒ¹é…ç¤ºä¾‹
    if total_matches > 0:
        print(f"\nğŸ‰ ã€åŒ¹é…ç»“æœç¤ºä¾‹ã€‘")
        
        # æ˜¾ç¤ºAPIæˆåŠŸçš„åŒ¹é…
        api_success_matches = [r for r in matching_results if r.get("match") and not r.get("fallback")]
        if api_success_matches:
            print(f"\nâœ… APIæˆåŠŸåŒ¹é… ({len(api_success_matches)}ä¸ª):")
            for i, result in enumerate(api_success_matches[:3]):
                print(f"  {i+1}. {result['source_table']}.{result['source_field']} <-> "
                      f"{result['target_table']}.{result['target_field']}")
                print(f"     ç½®ä¿¡åº¦: {result['confidence']:.2f}, ç›¸ä¼¼åº¦: {result.get('similarity', 0):.2f}")
        
        # æ˜¾ç¤ºæ™ºèƒ½å›é€€åŒ¹é…
        fallback_matches = [r for r in matching_results if r.get("match") and r.get("fallback")]
        if fallback_matches:
            print(f"\nğŸ”„ æ™ºèƒ½å›é€€åŒ¹é… ({len(fallback_matches)}ä¸ª):")
            for i, result in enumerate(fallback_matches[:3]):
                print(f"  {i+1}. {result['source_table']}.{result['source_field']} <-> "
                      f"{result['target_table']}.{result['target_field']}")
                print(f"     ç½®ä¿¡åº¦: {result['confidence']:.2f}, ç›¸ä¼¼åº¦: {result.get('similarity', 0):.2f}")
    else:
        print(f"\nâš ï¸  æœªæ‰¾åˆ°åŒ¹é…ç»“æœ")
    
    print(f"\nğŸŠ è¶…ç¨³å®šåŒ¹é…å®éªŒå®Œæˆï¼")


if __name__ == "__main__":
    main()